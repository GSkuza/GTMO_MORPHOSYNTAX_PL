#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GTMÃ˜ Verdict Analyzer - Analizator WyrokÃ³w SÄ…dowych
====================================================

Kompleksowa analiza wynikÃ³w GTMÃ˜ dla wyrokÃ³w sÄ…dowych i dokumentÃ³w prawnych.
Wizualizuje kluczowe metryki i identyfikuje "smoking guns" - najsÅ‚absze logicznie fragmenty tekstu.

FunkcjonalnoÅ›ci:
- "EKG" Wyroku - przebieg DostÄ™pnoÅ›ci Semantycznej (SA)
- Identyfikacja blokÃ³w krytycznych (SA < 10%)
- Mapa cieplna ÅºrÃ³deÅ‚ chaosu (CI decomposition)
- Macierz korelacji metryk GTMÃ˜
- RozkÅ‚ad SA w dokumencie
- Eksport wynikÃ³w do CSV/JSON

UÅ¼ycie:
  python gtmo_verdict_analyzer.py   <path_to_analysis.json> [opcje]

PrzykÅ‚ad:
    python gtmo_verdict_analyzer.py ../gtmo_results/analysis_xyz/full_document.json --all
"""

import sys
import io

# Fix Windows console encoding for Unicode characters
if sys.platform == 'win32':
    try:
        if not sys.stdout.closed:
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        if not sys.stderr.closed:
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except (ValueError, AttributeError):
        pass

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
import argparse
import numpy as np
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Ustawienia wizualne
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (20, 6)
plt.rcParams['axes.titlesize'] = 16
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['font.size'] = 10

# Progi krytyczne
CRITICAL_THRESHOLD = 0.10   # SA < 10% = krytyczne
WARNING_THRESHOLD = 0.30    # SA < 30% = ostrzeÅ¼enie


class NaturalLanguageRecommendations:
    """
    Generuje rekomendacje w jÄ™zyku naturalnym BEZ Å¼argonu technicznego.

    TÅ‚umaczy metryki GTMÃ˜ (SA, CI, ambiguity) na praktyczne porady
    dla prawnikÃ³w i legislatorÃ³w.
    """

    def __init__(self, use_llm: bool = True, api_key: str = None):
        """
        Args:
            use_llm: Czy uÅ¼ywaÄ‡ LLM (Claude) dla generowania przykÅ‚adÃ³w
            api_key: Klucz API do Anthropic Claude (opcjonalny, z env)
        """
        self.use_llm = use_llm
        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')

        if self.use_llm and not self.api_key:
            print("âš ï¸  Brak klucza API - LLM wyÅ‚Ä…czony. Ustaw ANTHROPIC_API_KEY w .env")
            self.use_llm = False


    def translate_severity(self, sa: float) -> str:
        """TÅ‚umaczy SA na poziom problemu w jÄ™zyku naturalnym"""
        if sa < 0.10:
            return "ekstremalnie trudny do zrozumienia"
        elif sa < 0.20:
            return "bardzo trudny do zrozumienia"
        elif sa < 0.30:
            return "trudny do zrozumienia"
        elif sa < 0.40:
            return "Å›rednio czytelny, wymaga poprawy"
        elif sa < 0.50:
            return "akceptowalny, ale moÅ¼na uproÅ›ciÄ‡"
        else:
            return "czytelny"


    def identify_main_problem(self, ci_morph_pct: float, ci_synt_pct: float,
                             ci_sem_pct: float) -> tuple[str, str]:
        """
        Identyfikuje gÅ‚Ã³wny problem na podstawie dekompozycji CI

        Returns:
            (problem_short, problem_detailed)
        """
        problems = {
            'morphological': (ci_morph_pct,
                            "zbyt skomplikowane sÅ‚ownictwo",
                            "Za duÅ¼o trudnych wyrazÃ³w prawniczych i skomplikowanych form gramatycznych"),
            'syntactic': (ci_synt_pct,
                         "zdania za dÅ‚ugie i zagmatwane",
                         "Zdanie ma zbyt wiele zagnieÅ¼dÅ¼eÅ„, podrzÄ™dnych czÄ™Å›ci i wtrÄ…ceÅ„"),
            'semantic': (ci_sem_pct,
                        "niejasne znaczenie sÅ‚Ã³w",
                        "Wyrazy majÄ… wieloznaczne znaczenie lub nie sÄ… jasno zdefiniowane")
        }

        # ZnajdÅº dominujÄ…cy problem
        max_problem = max(problems.items(), key=lambda x: x[1][0])
        _problem_type, (_pct, short, detailed) = max_problem

        return short, detailed


    def generate_quick_fixes(self, sentence_data: dict) -> list[str]:
        """
        Generuje listÄ™ szybkich poprawek (bez Å¼argonu)

        Args:
            sentence_data: Dict z metrykami (SA, CI_decomp, ambiguity, depth, etc.)
        """
        fixes = []

        ci_morph = sentence_data.get('CI_morph_pct', 0)
        ci_synt = sentence_data.get('CI_synt_pct', 0)
        ci_sem = sentence_data.get('CI_sem_pct', 0)
        ambiguity = sentence_data.get('ambiguity', 0)
        depth = sentence_data.get('depth', 0)

        # Morfologia
        if ci_morph > 40:
            fixes.append("ZamieÅ„ trudne wyrazy prawnicze na prostsze, codzienne sÅ‚owa")
            fixes.append(f"Ogranicz uÅ¼ycie skomplikowanych form wyrazÃ³w (obecnie: {ci_morph:.0f}% problemu)")
        elif ci_morph > 30:
            fixes.append("UproÅ›Ä‡ niektÃ³re wyrazy - uÅ¼yj prostszych synonimÃ³w")

        # SkÅ‚adnia
        if ci_synt > 40:
            if depth > 12:
                fixes.append(f"Rozbij zdanie na 3-4 krÃ³tsze (obecna gÅ‚Ä™bokoÅ›Ä‡ skÅ‚adniowa: {depth:.0f} poziomÃ³w)")
            elif depth > 8:
                fixes.append(f"Rozbij zdanie na 2 krÃ³tsze (obecna gÅ‚Ä™bokoÅ›Ä‡: {depth:.0f} poziomÃ³w)")
            fixes.append("UsuÅ„ niepotrzebne wtrÄ…cenia i nawiasy")
        elif ci_synt > 30:
            fixes.append("Rozbij na 2 krÃ³tsze zdania")

        # Semantyka
        if ci_sem > 40:
            fixes.append("Dodaj definicje dla niejasnych terminÃ³w prawnych")
            fixes.append("Precyzuj znaczenie wieloznacznych wyraÅ¼eÅ„")
        elif ci_sem > 30:
            fixes.append("WyjaÅ›nij lub zdefiniuj kluczowe pojÄ™cia")

        # WieloznacznoÅ›Ä‡
        if ambiguity > 4:
            fixes.append(f"UsuÅ„ wieloznacznoÅ›ci - zdanie moÅ¼na zrozumieÄ‡ na {ambiguity:.1f} rÃ³Å¼nych sposobÃ³w")
        elif ambiguity > 3.5:
            fixes.append("Precyzuj sformuÅ‚owania, aby ograniczyÄ‡ moÅ¼liwe interpretacje")

        # JeÅ›li brak konkretnych problemÃ³w, ogÃ³lne rady
        if not fixes:
            fixes.append("UproÅ›Ä‡ strukturÄ™ zdania")
            fixes.append("UÅ¼yj prostszego jÄ™zyka")

        return fixes


    def generate_long_term_fixes(self, sentence_data: dict) -> list[str]:
        """Generuje listÄ™ gÅ‚Ä™bszych zmian dÅ‚ugoterminowych"""
        fixes = []

        sa = sentence_data.get('SA', 0)

        if sa < 0.20:
            fixes.append("Przepisz caÅ‚y artykuÅ‚ od podstaw, prostszym jÄ™zykiem")
            fixes.append("RozwaÅ¼ podziaÅ‚ na kilka krÃ³tszych artykuÅ‚Ã³w")
            fixes.append("Dodaj sekcjÄ™ z przykÅ‚adami praktycznymi")
        elif sa < 0.30:
            fixes.append("Przepisz artykuÅ‚ uÅ¼ywajÄ…c listy punktowanej zamiast dÅ‚ugiego zdania")
            fixes.append("Dodaj glosariusz z definicjami kluczowych pojÄ™Ä‡")
        else:
            fixes.append("WprowadÅº przykÅ‚ady zastosowania w komentarzu")
            fixes.append("Ujednolicaj terminologiÄ™ w caÅ‚ej ustawie")

        return fixes


    def generate_legal_risks(self, sentence_data: dict) -> str:
        """
        Generuje opis ryzyk prawnych w naturalnym jÄ™zyku

        Bazuje na classification, ambiguity, SA
        """
        classification = sentence_data.get('classification', 'UNKNOWN')
        ambiguity = sentence_data.get('ambiguity', 0)
        sa = sentence_data.get('SA', 0)

        risks = []

        # Klasyfikacja
        if classification == 'CHAOTIC_STRUCTURE':
            risks.append("ğŸ”´ WYSOKIE RYZYKO: Przepis chaotyczny i nieprzewidywalny")
        elif classification == 'BALANCED_NORM':
            risks.append("ğŸŸ¡ ÅšREDNIE RYZYKO: Przepis wywaÅ¼ony, ale wymaga poprawy")

        # WieloznacznoÅ›Ä‡
        if ambiguity > 5:
            risks.append(f"âš–ï¸ Ekstremalna wieloznacznoÅ›Ä‡ ({ambiguity:.1f} interpretacji) - moÅ¼e byÄ‡ zakwestionowany jako niekonstytucyjny (naruszenie zasady lex certa)")
        elif ambiguity > 4:
            risks.append("âš–ï¸ Wysoka wieloznacznoÅ›Ä‡ - rÃ³Å¼ne sÄ…dy mogÄ… interpretowaÄ‡ na rÃ³Å¼ne sposoby, co prowadzi do chaosu orzeczniczego")
        elif ambiguity > 3:
            risks.append("âš–ï¸ MoÅ¼liwe spory interpretacyjne miÄ™dzy stronami postÄ™powania")

        # Niska dostÄ™pnoÅ›Ä‡ semantyczna
        if sa < 0.15:
            risks.append("ğŸ“‹ Organy stosujÄ…ce prawo nie bÄ™dÄ… wiedzieÄ‡ jak wykonaÄ‡ przepis - problemy w egzekucji")
        elif sa < 0.25:
            risks.append("ğŸ“‹ TrudnoÅ›ci w praktycznym stosowaniu - urzÄ™dnicy bÄ™dÄ… potrzebowaÄ‡ szczegÃ³Å‚owych wytycznych")

        # Ryzyko sporÃ³w
        if ambiguity > 3.5 and sa < 0.30:
            risks.append("ğŸ” Wysokie ryzyko dÅ‚ugotrwaÅ‚ych sporÃ³w sÄ…dowych o interpretacjÄ™")

        if not risks:
            risks.append("âš ï¸ Åšrednie ryzyko problemÃ³w z interpretacjÄ…")

        return " ".join(risks)


    def generate_recommendations(self, sentence_data: dict) -> dict:
        """
        GÅ‚Ã³wna metoda - generuje peÅ‚ny zestaw rekomendacji

        Args:
            sentence_data: Dict z metrykami zdania

        Returns:
            Dict z rekomendacjami w jÄ™zyku naturalnym
        """
        sa = sentence_data.get('SA', 0)
        text = sentence_data.get('text', '')

        # 1. Poziom problemu
        severity = self.translate_severity(sa)

        # 2. GÅ‚Ã³wny problem
        main_problem_short, main_problem_detailed = self.identify_main_problem(
            sentence_data.get('CI_morph_pct', 0),
            sentence_data.get('CI_synt_pct', 0),
            sentence_data.get('CI_sem_pct', 0)
        )

        # 3. Szybkie poprawki
        quick_fixes = self.generate_quick_fixes(sentence_data)

        # 4. DÅ‚ugoterminowe zmiany
        long_term = self.generate_long_term_fixes(sentence_data)

        # 5. Ryzyka prawne
        legal_risks = self.generate_legal_risks(sentence_data)

        # 6. PrzykÅ‚ad lepszej wersji (LLM lub szablon)
        if self.use_llm and len(text) > 50:
            example = self._generate_example_with_llm(sentence_data)
        else:
            example = self._generate_example_template(sentence_data)

        return {
            'severity': severity,
            'main_problem_short': main_problem_short,
            'main_problem_detailed': main_problem_detailed,
            'quick_fixes': quick_fixes,
            'long_term_fixes': long_term,
            'legal_risks': legal_risks,
            'example_better_version': example
        }


    def _generate_example_template(self, sentence_data: dict) -> str:
        """Generuje przykÅ‚ad poprawki uÅ¼ywajÄ…c szablonu (fallback)"""
        text = sentence_data.get('text', '')
        ci_synt = sentence_data.get('CI_synt_pct', 0)

        if ci_synt > 40:
            return f"""ByÅ‚o: "{text[:100]}..."

MoÅ¼e byÄ‡ (przykÅ‚ad uproszczenia):
"[Przepis naleÅ¼y rozbiÄ‡ na 2-3 krÃ³tsze zdania. KaÅ¼de zdanie powinno mieÄ‡ jeden gÅ‚Ã³wny cel.]"

Uwaga: To szablon - dla precyzyjnego przykÅ‚adu uÅ¼yj trybu z LLM."""
        else:
            return f"""ByÅ‚o: "{text[:100]}..."

MoÅ¼e byÄ‡: "[UproÅ›Ä‡ sÅ‚ownictwo i strukturÄ™ zdania]"

Uwaga: Dla konkretnego przykÅ‚adu wÅ‚Ä…cz tryb LLM."""


    def _generate_example_with_llm(self, sentence_data: dict) -> str:
        """Generuje szczegÃ³Å‚owÄ… analizÄ™ i rekomendacje uÅ¼ywajÄ…c Claude API"""
        try:
            import anthropic

            client = anthropic.Anthropic(api_key=self.api_key)

            text = sentence_data.get('text', '')
            sa = sentence_data.get('SA', 0)
            ci_morph_pct = sentence_data.get('CI_morph_pct', 0)
            ci_synt_pct = sentence_data.get('CI_synt_pct', 0)
            ci_sem_pct = sentence_data.get('CI_sem_pct', 0)
            ambiguity = sentence_data.get('ambiguity', 0)
            depth = sentence_data.get('depth', 0)

            prompt = f"""JesteÅ› ekspertem od przystÄ™pnoÅ›ci jÄ™zyka prawnego i analizy konstytucyjnej.

ZADANIE: Przeanalizuj poniÅ¼szy przepis i wygeneruj SZCZEGÃ“ÅOWÄ„ rekomendacjÄ™ w jÄ™zyku naturalnym (bez Å¼argonu technicznego).

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ORYGINALNY PRZEPIS:
"{text}"

METRYKI GTMÃ˜ (dla kontekstu - NIE pokazuj ich uÅ¼ytkownikowi w formie liczb):
â€¢ DostÄ™pnoÅ›Ä‡ Semantyczna (SA): {sa*100:.1f}%
â€¢ Dekompozycja NiedefinitywnoÅ›ci (CI):
  - Morfologiczna (trudne sÅ‚owa): {ci_morph_pct:.0f}%
  - SkÅ‚adniowa (dÅ‚ugie zdania): {ci_synt_pct:.0f}%
  - Semantyczna (niejasne znaczenie): {ci_sem_pct:.0f}%
â€¢ WieloznacznoÅ›Ä‡: {ambiguity:.1f} moÅ¼liwych interpretacji
â€¢ GÅ‚Ä™bokoÅ›Ä‡ skÅ‚adniowa: {depth:.0f} poziomÃ³w zagnieÅ¼dÅ¼enia

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INSTRUKCJE FORMATOWANIA:

1. **UÅ¼yj wizualnych ramek** z Unicode:
   - GÅ‚Ã³wne sekcje: â•”â•â•â•â•â•â•â•—, â•‘, â•šâ•â•â•â•â•â•â•
   - Podsekcje: â”â”â”â”â”“, â”ƒ, â”—â”â”â”â”›
   - Listy: â”œâ”€, â”‚, â””â”€

2. **Struktura odpowiedzi** (OBOWIÄ„ZKOWA):

   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   â•‘ ğŸ“‹ ANALIZA SEMANTYCZNA                                                    â•‘
   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   [WyjaÅ›nij JÄ˜ZYKIEM NATURALNYM dlaczego ten przepis jest trudny do zrozumienia.
    Bazuj na dekompozycji CI, ale NIE uÅ¼ywaj Å¼argonu - wyjaÅ›nij konkretnie co jest nie tak:
    - JeÅ›li CI_morph dominuje: "Za duÅ¼o skomplikowanych wyrazÃ³w prawniczych"
    - JeÅ›li CI_synt dominuje: "Zdanie zbyt dÅ‚ugie, za duÅ¼o wtrÄ…ceÅ„ i podrzÄ™dnikÃ³w"
    - JeÅ›li CI_sem dominuje: "Wyrazy majÄ… niejasne lub wieloznaczne znaczenie"]

   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   â•‘ ğŸ” MOÅ»LIWE INTERPRETACJE (problem wieloznacznoÅ›ci)                        â•‘
   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   [Opisz 3-7 rÃ³Å¼nych sposobÃ³w, w jakie ten przepis moÅ¼e byÄ‡ zrozumiany.
    Ponumeruj je: 1ï¸âƒ£, 2ï¸âƒ£, 3ï¸âƒ£, itd.
    Dla kaÅ¼dej interpretacji:
    - WyjaÅ›nij JAK moÅ¼na to zrozumieÄ‡
    - Napisz JAKI byÅ‚by skutek prawny tej interpretacji
    - WskaÅ¼ KTÃ“RY fragment przepisu powoduje tÄ™ wieloznacznoÅ›Ä‡]

   PrzykÅ‚ad:
   1ï¸âƒ£ Pierwsza interpretacja: [opis]
      â†’ Skutek prawny: [co by to oznaczaÅ‚o?]
      â†’ Å¹rÃ³dÅ‚o problemu: [ktÃ³ry fragment?]

   2ï¸âƒ£ Druga interpretacja: [opis]
      â†’ Skutek prawny: [...]
      â†’ Å¹rÃ³dÅ‚o problemu: [...]

   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   â•‘ ğŸ› ï¸ PROPOZYCJE NAPRAWY                                                     â•‘
   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   [Zaproponuj 2-3 WARIANTY poprawki. KaÅ¼dy wariant to KONKRETNY przepisany tekst.]

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
   â”ƒ WARIANT A: [nazwa, np. "PodziaÅ‚ na krÃ³tsze zdania"]                      â”ƒ
   â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›

   ğŸ“ Przepisany tekst:
   [Konkretny przepisany tekst - peÅ‚na wersja artykuÅ‚u po poprawce]

   âœ… Zalety:
   â”œâ”€ [zaleta 1]
   â”œâ”€ [zaleta 2]
   â””â”€ [zaleta 3]

   âš ï¸ Wady:
   â”œâ”€ [wada 1 lub "Brak istotnych wad"]
   â””â”€ [wada 2]

   ğŸ“ˆ Szacowany wzrost SA: [np. "z 23% â†’ ~55%"] (zwiÄ™kszenie czytelnoÅ›ci o ~140%)

   ğŸ¯ Rekomendacja: [Czy zalecasz ten wariant? Dla kogo jest najlepszy?]

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
   â”ƒ WARIANT B: [nazwa, np. "Lista punktowana + definicje"]                   â”ƒ
   â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›

   [PowtÃ³rz strukturÄ™ jak w WARIANT A]

   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   â•‘ âš–ï¸ RYZYKO PRAWNE                                                          â•‘
   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   [Opisz KONKRETNE ryzyka:
    - JeÅ›li SA < 15%: "Organy stosujÄ…ce prawo nie bÄ™dÄ… wiedziaÅ‚y jak wykonaÄ‡ przepis"
    - JeÅ›li wieloznacznoÅ›Ä‡ > 4: "RÃ³Å¼ne sÄ…dy mogÄ… interpretowaÄ‡ na rÃ³Å¼ne sposoby - chaos orzeczniczy"
    - JeÅ›li CI_sem dominuje: "Ryzyko zakwestionowania jako niekonstytucyjny (naruszenie lex certa)"

    NIE uÅ¼ywaj metrycznych wartoÅ›ci - pisz JÄ˜ZYKIEM NATURALNYM o skutkach prawnych]

   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   â•‘ ğŸ¯ RANKING PRIORYTETÃ“W                                                    â•‘
   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   1. [NajwaÅ¼niejsze dziaÅ‚anie - co zrobiÄ‡ TERAZ]
   2. [Drugie w kolejnoÅ›ci]
   3. [Trzecie w kolejnoÅ›ci]

   â±ï¸ Szacowany czas wdroÅ¼enia: [np. "2-3 dni robocze dla legislatora"]
   ğŸ’° Koszty: [np. "Niskie - wymaga tylko przeformuÅ‚owania"]

WAÅ»NE ZASADY:
- NIE uÅ¼ywaj Å¼argonu technicznego (SA, CI, D-S-E, entropia, itp.) w odpowiedzi
- Pisz tak, Å¼eby zrozumiaÅ‚ prawnik-praktyk lub urzÄ™dnik
- WyjaÅ›nij DLACZEGO coÅ› jest problemem, a nie JAKI ma wskaÅºnik
- Wszystkie propozycje poprawek muszÄ… zachowaÄ‡ treÅ›Ä‡ normatywnÄ…
- UÅ¼ywaj emojis do wizualizacji (ğŸ“‹, ğŸ”, ğŸ› ï¸, âœ…, âš ï¸, ğŸ“ˆ, ğŸ¯, â±ï¸, ğŸ’°)
- Ramki Unicode muszÄ… byÄ‡ DOKÅADNIE wyrÃ³wnane

TERAZ WYGENERUJ PEÅNÄ„ ANALIZÄ˜:"""

            message = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=3000,
                temperature=0.4,
                messages=[{"role": "user", "content": prompt}]
            )

            return message.content[0].text

        except Exception as e:
            print(f"âš ï¸  LLM error: {e}")
            return self._generate_example_template(sentence_data)


class GTMOVerdictAnalyzer:
    """Analizator wyrokÃ³w GTMÃ˜"""

    def __init__(self, json_path: str):
        """
        Inicjalizacja analizatora

        Args:
            json_path: ÅšcieÅ¼ka do pliku JSON z analizÄ… GTMÃ˜
        """
        self.json_path = Path(json_path)
        self.output_dir = self.json_path.parent / "verdict_analysis_output"
        self.output_dir.mkdir(exist_ok=True)

        print(f"ğŸ“‚ Wczytywanie: {self.json_path.name}")

        # Wczytaj i przetworz dane
        self.raw_data = self._load_json()
        self.analyses = self._extract_analyses()
        self.df = self._create_dataframe()

        print(f"âœ“ ZaÅ‚adowano {len(self.df)} blokÃ³w tekstowych")
        print(f"ğŸ’¾ Wyniki bÄ™dÄ… zapisane w: {self.output_dir}")


    def _load_json(self) -> dict:
        """Wczytuje plik JSON"""
        with open(self.json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data


    def _extract_analyses(self) -> List[dict]:
        """
        WyciÄ…ga analizy z rÃ³Å¼nych moÅ¼liwych struktur JSON

        Returns:
            Lista analiz blokÃ³w tekstowych
        """
        data = self.raw_data
        analyses = []

        print(f"\nğŸ” Analiza struktury JSON...")
        print(f"   Typ gÅ‚Ã³wnego obiektu: {type(data)}")

        if isinstance(data, list):
            # Format: lista analiz
            print(f"   âœ“ Wykryto LISTÄ˜ z {len(data)} analizami")
            analyses = data

        elif isinstance(data, dict):
            print(f"   DostÄ™pne klucze: {list(data.keys())}")

            # SprawdÅº rÃ³Å¼ne moÅ¼liwe klucze
            if 'sentences' in data:
                print(f"   âœ“ Wykryto strukturÄ™ z kluczem 'sentences'")
                analyses = data['sentences']
            elif 'analyses' in data:
                print(f"   âœ“ Wykryto strukturÄ™ z kluczem 'analyses'")
                analyses = data['analyses']
            elif 'results' in data:
                print(f"   âœ“ Wykryto strukturÄ™ z kluczem 'results'")
                analyses = data['results']
            elif 'articles' in data:
                print(f"   âœ“ Wykryto strukturÄ™ z kluczem 'articles'")
                # WyciÄ…gnij zdania z kaÅ¼dego artykuÅ‚u
                articles = data['articles']
                sentences = []
                for article in articles:
                    if isinstance(article, dict) and 'sentences' in article:
                        sentences.extend(article['sentences'])
                if sentences:
                    print(f"   âœ“ Wyekstrahowano {len(sentences)} zdaÅ„ z {len(articles)} artykuÅ‚Ã³w")
                    analyses = sentences
                else:
                    # Fallback: jeÅ›li articles nie majÄ… sentences, uÅ¼yj articles jako blokÃ³w
                    analyses = articles
            elif '_original_data' in data and isinstance(data['_original_data'], dict):
                if 'articles' in data['_original_data']:
                    print(f"   âœ“ Wykryto strukturÄ™ '_original_data/articles'")
                    analyses = data['_original_data']['articles']
            elif 'stanza_analysis' in data and isinstance(data['stanza_analysis'], dict):
                if 'sentences' in data['stanza_analysis']:
                    print(f"   âœ“ Wykryto strukturÄ™ 'stanza_analysis/sentences'")
                    analyses = data['stanza_analysis']['sentences']
            elif 'content' in data and 'coordinates' in data:
                # Pojedyncza analiza
                print(f"   âœ“ Wykryto POJEDYNCZÄ„ analizÄ™")
                analyses = [data]
            else:
                # Szukaj list wewnÄ…trz
                for key, value in data.items():
                    if isinstance(value, list) and len(value) > 0:
                        if isinstance(value[0], dict) and 'content' in value[0]:
                            print(f"   âœ“ Wykryto analizy pod kluczem '{key}'")
                            analyses = value
                            break

        if not analyses:
            raise ValueError(
                "âŒ Nie znaleziono analiz w pliku JSON!\n"
                "Oczekiwano:\n"
                "- listy analiz, lub\n"
                "- obiektu z kluczem 'sentences'/'analyses'/'results', lub\n"
                "- obiektu z '_original_data/articles', lub\n"
                "- obiektu z 'stanza_analysis/sentences'"
            )

        return analyses


    def _safe_get(self, obj, *keys, default=0):
        """Bezpiecznie wyciÄ…ga wartoÅ›Ä‡ z zagnieÅ¼dÅ¼onej struktury"""
        for key in keys:
            if isinstance(obj, dict) and key in obj:
                obj = obj[key]
            else:
                return default
        return obj if obj is not None else default


    def _create_dataframe(self) -> pd.DataFrame:
        """
        Przetwarza analizy na pandas DataFrame

        Returns:
            DataFrame z przetworzonymi danymi
        """
        print("\nğŸ”„ Przetwarzanie danych...")

        processed_data = []
        errors = []

        for i, analysis in enumerate(self.analyses):
            try:
                # WyciÄ…gnij dane z rÃ³Å¼nych lokalizacji
                content = self._safe_get(analysis, 'content', default={})
                coords = self._safe_get(analysis, 'coordinates', default={})
                const_metrics = self._safe_get(analysis, 'constitutional_metrics', default={})
                additional_metrics = self._safe_get(analysis, 'additional_metrics', default={})
                depth_metrics = self._safe_get(analysis, 'depth_metrics', default={})
                rhetorical = self._safe_get(analysis, 'rhetorical_analysis', default={})

                # SprawdÅº teÅ¼ _original_data
                original_data = self._safe_get(analysis, '_original_data', default={})
                original_coords = self._safe_get(original_data, 'coordinates', default={})
                original_const_metrics = self._safe_get(original_data, 'constitutional_metrics', default={})

                # Tekst
                text = self._safe_get(analysis, 'text') or self._safe_get(content, 'text', default='')
                text_preview = (text[:150] + '...') if len(text) > 150 else text

                # WspÃ³Å‚rzÄ™dne D-S-E
                D = self._safe_get(coords, 'determination') or self._safe_get(original_coords, 'determination') or self._safe_get(analysis, 'gtmo_coordinates', 'determination')
                S = self._safe_get(coords, 'stability') or self._safe_get(original_coords, 'stability') or self._safe_get(analysis, 'gtmo_coordinates', 'stability')
                E = self._safe_get(coords, 'entropy') or self._safe_get(original_coords, 'entropy') or self._safe_get(analysis, 'gtmo_coordinates', 'entropy')

                # Metryki konstytucyjne
                SA_obj = self._safe_get(const_metrics, 'semantic_accessibility', default={}) or self._safe_get(original_const_metrics, 'semantic_accessibility', default={})
                # FIXED: ObsÅ‚uga nowego formatu z v2/v3 oraz starego formatu z 'value'
                if isinstance(SA_obj, dict):
                    # Preferuj v3 (nowsze), potem v2, potem stary format
                    SA = (self._safe_get(SA_obj, 'v3', 'value') or
                          self._safe_get(SA_obj, 'v2', 'value') or
                          self._safe_get(SA_obj, 'value'))
                else:
                    SA = SA_obj

                CD_obj = self._safe_get(const_metrics, 'definiteness', default={}) or self._safe_get(original_const_metrics, 'definiteness', default={})
                CD = self._safe_get(CD_obj, 'value') if isinstance(CD_obj, dict) else CD_obj

                CI_obj = self._safe_get(const_metrics, 'indefiniteness', default={}) or self._safe_get(original_const_metrics, 'indefiniteness', default={})
                CI = self._safe_get(CI_obj, 'value') if isinstance(CI_obj, dict) else CI_obj

                # Dekompozycja CI
                decomp = self._safe_get(CI_obj, 'decomposition', default={}) if isinstance(CI_obj, dict) else {}
                CI_morph_pct = self._safe_get(decomp, 'morphological', 'percentage')
                CI_synt_pct = self._safe_get(decomp, 'syntactic', 'percentage')
                CI_sem_pct = self._safe_get(decomp, 'semantic', 'percentage')

                # GÅ‚Ä™bokoÅ›Ä‡ i ambiguity
                depth = self._safe_get(depth_metrics, 'max_depth') or self._safe_get(analysis, 'depth') or self._safe_get(original_data, 'depth')
                ambiguity = self._safe_get(additional_metrics, 'ambiguity') or self._safe_get(analysis, 'ambiguity')

                # Klasyfikacja
                classification_obj = self._safe_get(const_metrics, 'classification', default={}) or self._safe_get(original_const_metrics, 'classification', default={})
                classification = self._safe_get(classification_obj, 'type', default='UNKNOWN')

                # Analiza retoryczna
                pos_anomalies = self._safe_get(rhetorical, 'pos_anomalies', default={})
                pos_anomaly_score = self._safe_get(pos_anomalies, 'anomaly_score')

                # Numer zdania
                sentence_num = self._safe_get(analysis, 'sentence_number') or self._safe_get(analysis, 'analysis_metadata', 'sentence_number') or (i + 1)

                block_data = {
                    'block_id': i,
                    'sentence_number': sentence_num,
                    'text': text_preview,
                    'full_text': text,
                    'D': float(D) if D else 0.0,
                    'S': float(S) if S else 0.0,
                    'E': float(E) if E else 0.0,
                    'SA': float(SA) if SA else 0.0,
                    'CD': float(CD) if CD else 0.0,
                    'CI': float(CI) if CI else 0.0,
                    'depth': float(depth) if depth else 0.0,
                    'ambiguity': float(ambiguity) if ambiguity else 0.0,
                    'classification': str(classification),
                    'CI_morph_pct': float(CI_morph_pct) if CI_morph_pct else 0.0,
                    'CI_synt_pct': float(CI_synt_pct) if CI_synt_pct else 0.0,
                    'CI_sem_pct': float(CI_sem_pct) if CI_sem_pct else 0.0,
                    'pos_anomaly_score': float(pos_anomaly_score) if pos_anomaly_score else 0.0
                }

                processed_data.append(block_data)

            except Exception as e:
                errors.append(f"Blok {i}: {str(e)}")
                continue

        if not processed_data:
            raise ValueError(f"âŒ Nie udaÅ‚o siÄ™ przetworzyÄ‡ Å¼adnych danych! BÅ‚Ä™dy: {errors[:5]}")

        df = pd.DataFrame(processed_data)

        # Raport
        print(f"   âœ“ Przetworzono: {len(df)} blokÃ³w")
        if errors:
            print(f"   âš  BÅ‚Ä™dy: {len(errors)} blokÃ³w")

        return df


    # ========================================================================
    # STATYSTYKI OGÃ“LNE
    # ========================================================================

    def print_statistics(self):
        """WyÅ›wietla statystyki ogÃ³lne dokumentu"""
        print("\n" + "=" * 70)
        print("STATYSTYKI OGÃ“LNE DOKUMENTU")
        print("=" * 70)

        # Filtruj wartoÅ›ci niezerowe
        sa_nonzero = self.df[self.df['SA'] > 0]['SA']
        depth_nonzero = self.df[self.df['depth'] > 0]['depth']

        if len(sa_nonzero) > 0:
            print(f"\nğŸ“Š DostÄ™pnoÅ›Ä‡ Semantyczna (SA):")
            print(f"   â€¢ Åšrednia: {sa_nonzero.mean()*100:.2f}%")
            print(f"   â€¢ Odchylenie std: {sa_nonzero.std()*100:.2f}%")
            print(f"   â€¢ Min (najgorszy): {sa_nonzero.min()*100:.2f}%")
            print(f"   â€¢ Max (najlepszy): {sa_nonzero.max()*100:.2f}%")
            print(f"   â€¢ BlokÃ³w z SA > 0: {len(sa_nonzero)}/{len(self.df)}")

        if len(depth_nonzero) > 0:
            print(f"\nğŸ“Š GÅ‚Ä™bokoÅ›Ä‡ SkÅ‚adniowa:")
            print(f"   â€¢ Åšrednia: {depth_nonzero.mean():.1f}")
            print(f"   â€¢ Maksymalna: {depth_nonzero.max():.0f}")

        # WspÃ³Å‚rzÄ™dne D-S-E
        d_nonzero = self.df[self.df['D'] > 0]['D']
        s_nonzero = self.df[self.df['S'] > 0]['S']
        e_nonzero = self.df[self.df['E'] > 0]['E']

        if len(d_nonzero) > 0:
            print(f"\nğŸ“Š WspÃ³Å‚rzÄ™dne D-S-E:")
            print(f"   â€¢ Determination: Î¼={d_nonzero.mean():.3f}, Ïƒ={d_nonzero.std():.3f}")
            print(f"   â€¢ Stability: Î¼={s_nonzero.mean():.3f}, Ïƒ={s_nonzero.std():.3f}")
            print(f"   â€¢ Entropy: Î¼={e_nonzero.mean():.3f}, Ïƒ={e_nonzero.std():.3f}")

        # Klasyfikacje
        print(f"\nğŸ“Š Klasyfikacja strukturalna:")
        for cls, count in self.df['classification'].value_counts().items():
            pct = (count / len(self.df)) * 100
            print(f"   â€¢ {cls}: {count} blokÃ³w ({pct:.1f}%)")

        # Å¹rÃ³dÅ‚a chaosu
        ci_components = self.df[['CI_morph_pct', 'CI_synt_pct', 'CI_sem_pct']]
        ci_nonzero = ci_components[(ci_components > 0).any(axis=1)]

        if len(ci_nonzero) > 0:
            print(f"\nğŸ“Š Åšredni rozkÅ‚ad ÅºrÃ³deÅ‚ NiedefinitywnoÅ›ci (CI):")
            print(f"   â€¢ Morfologiczna: {ci_nonzero['CI_morph_pct'].mean():.1f}%")
            print(f"   â€¢ SkÅ‚adniowa: {ci_nonzero['CI_synt_pct'].mean():.1f}%")
            print(f"   â€¢ Semantyczna: {ci_nonzero['CI_sem_pct'].mean():.1f}%")


    # ========================================================================
    # WIZUALIZACJA 1: "EKG" WYROKU
    # ========================================================================

    def visualize_ekg(self):
        """Wykres liniowy DostÄ™pnoÅ›ci Semantycznej - "EKG" wyroku"""
        print(f"\nğŸ“ˆ Generowanie wykresu 'EKG' wyroku...")

        df_plot = self.df[self.df['SA'] > 0].copy()

        if len(df_plot) == 0:
            print("   âŒ Brak danych SA do wizualizacji!")
            return

        df_plot['SA_formatted'] = df_plot['SA'].apply(lambda x: f"{x*100:.2f}%")

        fig = px.line(
            df_plot,
            x='block_id',
            y='SA',
            title=f'Przebieg DostÄ™pnoÅ›ci Semantycznej (SA) w dokumencie - "EKG" Wyroku<br><sub>Analiza {len(df_plot)} blokÃ³w tekstowych</sub>',
            labels={
                'block_id': 'Numer Bloku Tekstowego',
                'SA': 'DostÄ™pnoÅ›Ä‡ Semantyczna (SA)'
            },
            hover_data={
                'SA': False,
                'SA_formatted': True,
                'text': True,
                'sentence_number': True,
                'depth': True,
                'classification': True
            }
        )

        # Progi
        fig.add_hline(
            y=CRITICAL_THRESHOLD,
            line_dash="dash",
            line_color="red",
            line_width=2,
            annotation_text=f"PrÃ³g Krytyczny ({CRITICAL_THRESHOLD*100}%)",
            annotation_position="right"
        )

        fig.add_hline(
            y=WARNING_THRESHOLD,
            line_dash="dot",
            line_color="orange",
            line_width=1,
            annotation_text=f"PrÃ³g Ostrzegawczy ({WARNING_THRESHOLD*100}%)",
            annotation_position="right"
        )

        fig.update_traces(
            mode='lines+markers',
            line=dict(width=2, color='steelblue'),
            marker=dict(size=6)
        )

        fig.update_layout(
            yaxis_tickformat='.0%',
            height=600,
            hovermode='closest'
        )

        output_path = self.output_dir / "ekg_wyroku.html"
        fig.write_html(str(output_path))

        print(f"   âœ“ Zapisano: {output_path.name}")


    # ========================================================================
    # WIZUALIZACJA 2: SMOKING GUNS
    # ========================================================================

    def identify_smoking_guns(self):
        """Identyfikuje i raportuje bloki krytyczne"""
        print(f"\nğŸ” Identyfikacja 'Smoking Guns'...")

        df_valid = self.df[self.df['SA'] > 0].copy()

        critical_blocks = df_valid[df_valid['SA'] < CRITICAL_THRESHOLD].sort_values('SA')
        warning_blocks = df_valid[
            (df_valid['SA'] >= CRITICAL_THRESHOLD) &
            (df_valid['SA'] < WARNING_THRESHOLD)
        ].sort_values('SA')

        print("\n" + "=" * 70)
        print("ğŸ” SMOKING GUNS - BLOKI KRYTYCZNE")
        print("=" * 70)

        if len(critical_blocks) == 0:
            print(f"\nâœ“ Dobra wiadomoÅ›Ä‡: Brak blokÃ³w krytycznych (SA < {CRITICAL_THRESHOLD*100}%)")
        else:
            print(f"\nâŒ Znaleziono {len(critical_blocks)} blokÃ³w krytycznych (SA < {CRITICAL_THRESHOLD*100}%):")
            print(f"   To {(len(critical_blocks)/len(df_valid)*100):.1f}% dokumentu!\n")

            # Top 5
            print(f"ğŸ”´ TOP 5 NAJGORSZYCH BLOKÃ“W:\n")
            for idx, row in critical_blocks.head(5).iterrows():
                print(f"#{row['block_id']} | SA: {row['SA']*100:.2f}% | GÅ‚Ä™bokoÅ›Ä‡: {row['depth']:.0f}")
                print(f"   {row['text'][:100]}...\n")

            # Zapisz do pliku
            report_path = self.output_dir / "smoking_guns.txt"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("SMOKING GUNS - BLOKI KRYTYCZNE\n")
                f.write("=" * 70 + "\n\n")
                f.write(f"Znaleziono: {len(critical_blocks)} blokÃ³w krytycznych\n")
                f.write(f"PrÃ³g: SA < {CRITICAL_THRESHOLD*100}%\n\n")

                for idx, row in critical_blocks.iterrows():
                    f.write(f"\nBlok #{row['block_id']} (Zdanie #{row['sentence_number']})\n")
                    f.write(f"SA: {row['SA']*100:.2f}% | GÅ‚Ä™bokoÅ›Ä‡: {row['depth']:.0f} | Chaos skÅ‚adni: {row['CI_synt_pct']:.1f}%\n")
                    f.write(f"Klasyfikacja: {row['classification']}\n")
                    f.write(f"Tekst: {row['full_text']}\n")
                    f.write("-" * 70 + "\n")

            print(f"   ğŸ’¾ Raport zapisany: {report_path.name}")

            # CSV
            csv_path = self.output_dir / "smoking_guns.csv"
            critical_blocks.to_csv(csv_path, index=False, encoding='utf-8')
            print(f"   ğŸ’¾ CSV zapisany: {csv_path.name}")

        if len(warning_blocks) > 0:
            print(f"\nâš ï¸  Znaleziono {len(warning_blocks)} blokÃ³w ostrzegawczych ({CRITICAL_THRESHOLD*100}% â‰¤ SA < {WARNING_THRESHOLD*100}%)")
            print(f"   To {(len(warning_blocks)/len(df_valid)*100):.1f}% dokumentu")

        return critical_blocks, warning_blocks


    # ========================================================================
    # WIZUALIZACJA 3: MAPA CIEPLNA CHAOSU
    # ========================================================================

    def visualize_chaos_heatmap(self):
        """Mapa cieplna ÅºrÃ³deÅ‚ chaosu (CI decomposition)"""
        print(f"\nğŸ”¥ Generowanie mapy cieplnej chaosu...")

        chaos_df = self.df[
            (self.df['CI_morph_pct'] > 0) |
            (self.df['CI_synt_pct'] > 0) |
            (self.df['CI_sem_pct'] > 0)
        ].copy()

        if len(chaos_df) == 0:
            print("   âŒ Brak danych o dekompozycji CI!")
            return

        chaos_components = chaos_df[['CI_morph_pct', 'CI_synt_pct', 'CI_sem_pct']]

        plt.figure(figsize=(22, 5))

        sns.heatmap(
            chaos_components.T,
            cmap='Reds',
            annot=False,
            cbar_kws={'label': 'UdziaÅ‚ procentowy (%)'},
            vmin=0,
            vmax=100
        )

        plt.title(
            f'Mapa Cieplna Å¹rÃ³deÅ‚ Chaosu (NiedefinitywnoÅ›ci CI)\n'
            f'Analiza {len(chaos_df)} blokÃ³w',
            pad=20
        )
        plt.xlabel('Numer Bloku', fontsize=12)
        plt.ylabel('Komponent Chaosu', fontsize=12)
        plt.yticks(
            ticks=[0.5, 1.5, 2.5],
            labels=['Morfologiczny', 'SkÅ‚adniowy', 'Semantyczny'],
            rotation=0
        )

        plt.tight_layout()

        output_path = self.output_dir / "chaos_heatmap.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"   âœ“ Zapisano: {output_path.name}")

        # Statystyki dominacji
        dominant = chaos_components.idxmax(axis=1).value_counts()
        print(f"\n   ğŸ“Š DominujÄ…ce ÅºrÃ³dÅ‚o chaosu:")
        source_names = {
            'CI_morph_pct': 'Morfologiczny',
            'CI_synt_pct': 'SkÅ‚adniowy',
            'CI_sem_pct': 'Semantyczny'
        }
        for source, count in dominant.items():
            pct = (count / len(chaos_df)) * 100
            print(f"      â€¢ {source_names.get(source, source)}: {count} blokÃ³w ({pct:.1f}%)")


    # ========================================================================
    # WIZUALIZACJA 4: MACIERZ KORELACJI
    # ========================================================================

    def visualize_correlation(self):
        """Macierz korelacji metryk GTMÃ˜"""
        print(f"\nğŸ”— Generowanie macierzy korelacji...")

        corr_cols = [
            'SA', 'D', 'S', 'E',
            'CD', 'CI',
            'depth', 'ambiguity',
            'CI_morph_pct', 'CI_synt_pct', 'CI_sem_pct',
            'pos_anomaly_score'
        ]

        available_cols = [col for col in corr_cols if col in self.df.columns and self.df[col].sum() != 0]

        if len(available_cols) < 3:
            print("   âŒ Za maÅ‚o danych do obliczenia korelacji!")
            return

        corr_matrix = self.df[available_cols].corr()

        plt.figure(figsize=(14, 12))

        sns.heatmap(
            corr_matrix,
            annot=True,
            cmap='coolwarm',
            fmt='.2f',
            center=0,
            vmin=-1,
            vmax=1,
            square=True,
            linewidths=0.5,
            cbar_kws={'label': 'WspÃ³Å‚czynnik korelacji Pearsona'}
        )

        plt.title(
            'Macierz Korelacji Kluczowych Metryk GTMÃ˜\n'
            'WartoÅ›ci bliskie -1 lub +1 oznaczajÄ… silny zwiÄ…zek',
            pad=20
        )
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)

        plt.tight_layout()

        output_path = self.output_dir / "correlation_matrix.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"   âœ“ Zapisano: {output_path.name}")

        # Najsilniejsze korelacje z SA
        if 'SA' in corr_matrix.columns:
            sa_corrs = corr_matrix['SA'].drop('SA').sort_values()
            print(f"\n   ğŸ“Š Najsilniejsze korelacje z SA:")
            print(f"      Negatywne (â†‘metryka â†’ â†“SA):")
            for metric, corr in sa_corrs.head(3).items():
                print(f"         â€¢ {metric}: {corr:.3f}")
            print(f"      Pozytywne (â†‘metryka â†’ â†‘SA):")
            for metric, corr in sa_corrs.tail(3).items():
                print(f"         â€¢ {metric}: {corr:.3f}")


    # ========================================================================
    # WIZUALIZACJA 5: ROZKÅAD SA
    # ========================================================================

    def visualize_sa_distribution(self):
        """RozkÅ‚ad SA w dokumencie"""
        print(f"\nğŸ“Š Generowanie rozkÅ‚adu SA...")

        df_valid = self.df[self.df['SA'] > 0].copy()

        if len(df_valid) == 0:
            print("   âŒ Brak danych SA!")
            return

        fig, axes = plt.subplots(1, 2, figsize=(18, 6))

        # Histogram
        axes[0].hist(df_valid['SA'] * 100, bins=30, edgecolor='black', alpha=0.7, color='steelblue')
        axes[0].axvline(x=CRITICAL_THRESHOLD*100, color='red', linestyle='--', linewidth=2, label=f'Krytyczny ({CRITICAL_THRESHOLD*100}%)')
        axes[0].axvline(x=WARNING_THRESHOLD*100, color='orange', linestyle='--', linewidth=2, label=f'Ostrzegawczy ({WARNING_THRESHOLD*100}%)')
        axes[0].set_xlabel('DostÄ™pnoÅ›Ä‡ Semantyczna SA (%)', fontsize=12)
        axes[0].set_ylabel('Liczba blokÃ³w', fontsize=12)
        axes[0].set_title('RozkÅ‚ad DostÄ™pnoÅ›ci Semantycznej', fontsize=14)
        axes[0].legend()
        axes[0].grid(alpha=0.3)

        # Box plot wedÅ‚ug klasyfikacji
        classifications = df_valid['classification'].unique()
        classification_data = [
            df_valid[df_valid['classification'] == cls]['SA'] * 100
            for cls in classifications
        ]

        bp = axes[1].boxplot(
            classification_data,
            labels=classifications,
            patch_artist=True,
            widths=0.6
        )

        # Kolory
        for patch in bp['boxes']:
            patch.set_facecolor('lightblue')

        axes[1].axhline(y=CRITICAL_THRESHOLD*100, color='red', linestyle='--', linewidth=2, alpha=0.5)
        axes[1].axhline(y=WARNING_THRESHOLD*100, color='orange', linestyle='--', linewidth=2, alpha=0.5)
        axes[1].set_ylabel('DostÄ™pnoÅ›Ä‡ Semantyczna SA (%)', fontsize=12)
        axes[1].set_xlabel('Klasyfikacja strukturalna', fontsize=12)
        axes[1].set_title('SA wedÅ‚ug typu struktury', fontsize=14)
        plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')
        axes[1].grid(alpha=0.3, axis='y')

        plt.tight_layout()

        output_path = self.output_dir / "sa_distribution.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"   âœ“ Zapisano: {output_path.name}")


    # ========================================================================
    # REKOMENDACJE W JÄ˜ZYKU NATURALNYM
    # ========================================================================

    def generate_natural_language_recommendations(self, use_llm: bool = True):
        """
        Generuje rekomendacje w jÄ™zyku naturalnym dla przepisÃ³w z SA < 30%

        Args:
            use_llm: Czy uÅ¼ywaÄ‡ LLM (Claude) dla generowania przykÅ‚adÃ³w poprawek
        """
        print(f"\nğŸ“ Generowanie rekomendacji w jÄ™zyku naturalnym...")

        # Filtruj zdania z SA < 30%
        problematic = self.df[self.df['SA'] < WARNING_THRESHOLD].copy()

        if len(problematic) == 0:
            print(f"   âœ“ Åšwietnie! Brak przepisÃ³w wymagajÄ…cych poprawy (SA < {WARNING_THRESHOLD*100}%)")
            return

        print(f"   ğŸ“Š Znaleziono {len(problematic)} przepisÃ³w wymagajÄ…cych poprawy ({len(problematic)/len(self.df)*100:.1f}% dokumentu)")

        # Inicjalizuj generator rekomendacji
        recommender = NaturalLanguageRecommendations(
            use_llm=use_llm,
            api_key=os.getenv('ANTHROPIC_API_KEY')
        )

        # Generuj rekomendacje
        recommendations = []

        for idx, row in problematic.iterrows():
            sentence_data = {
                'text': row['full_text'],
                'SA': row['SA'],
                'CI_morph_pct': row.get('CI_morph_pct', 0),
                'CI_synt_pct': row.get('CI_synt_pct', 0),
                'CI_sem_pct': row.get('CI_sem_pct', 0),
                'ambiguity': row.get('ambiguity', 0),
                'depth': row.get('depth', 0),
                'classification': row.get('classification', 'UNKNOWN')
            }

            # Generuj rekomendacje
            rec = recommender.generate_recommendations(sentence_data)

            recommendations.append({
                'sentence_id': row['block_id'],
                'sentence_number': row.get('sentence_number', idx),
                'text_preview': row['text'][:100],
                'full_text': row['full_text'],
                **rec
            })

            # Progress
            if (len(recommendations) % 20 == 0):
                print(f"   ğŸ”„ Przetworzono {len(recommendations)}/{len(problematic)} przepisÃ³w...")

        print(f"   âœ“ Wygenerowano {len(recommendations)} rekomendacji")

        # Zapisz raport
        self._save_recommendations_report(recommendations)

        return recommendations


    def _save_recommendations_report(self, recommendations: List[dict]):
        """Zapisuje raport rekomendacji w czytelnym formacie"""

        report_path = self.output_dir / "recommendations_natural_language.txt"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("RAPORT: PRZEPISY WYMAGAJÄ„CE POPRAWY\n")
            f.write("Wnioski w jÄ™zyku naturalnym (bez Å¼argonu technicznego)\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"Dokument: {self.json_path.parent.name}\n")
            f.write(f"ProblemÃ³w znalezionych: {len(recommendations)} przepisÃ³w\n\n")

            for i, rec in enumerate(recommendations, 1):
                f.write("-" * 80 + "\n")
                f.write(f"PRZEPIS #{i} (Zdanie #{rec['sentence_number']})\n")
                f.write("-" * 80 + "\n\n")

                f.write("TEKST:\n")
                f.write(f'"{rec["full_text"]}"\n\n')

                f.write("PROBLEM:\n")
                f.write(f"Ten przepis jest {rec['severity']}.\n")
                f.write(f"GÅ‚Ã³wny problem: {rec['main_problem_detailed']}\n\n")

                f.write("CO ZROBIÄ† TERAZ (proste poprawki):\n")
                for j, fix in enumerate(rec['quick_fixes'], 1):
                    f.write(f"  {j}. {fix}\n")
                f.write("\n")

                f.write("CO ZROBIÄ† DÅUGOTERMINOWO:\n")
                for j, fix in enumerate(rec['long_term_fixes'], 1):
                    f.write(f"  {j}. {fix}\n")
                f.write("\n")

                f.write("PRZYKÅAD LEPSZEJ WERSJI:\n")
                f.write(rec['example_better_version'])
                f.write("\n\n")

                f.write("RYZYKO PRAWNE:\n")
                f.write(rec['legal_risks'])
                f.write("\n\n")

        print(f"   ğŸ’¾ Raport zapisany: {report_path.name}")

        # Zapisz teÅ¼ JSON dla Å‚atwiejszego parsowania
        json_path = self.output_dir / "recommendations_natural_language.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(recommendations, f, indent=2, ensure_ascii=False)

        print(f"   ğŸ’¾ JSON zapisany: {json_path.name}")


    # ========================================================================
    # EKSPORT WYNIKÃ“W
    # ========================================================================

    def export_results(self):
        """Eksportuje wyniki do plikÃ³w"""
        print(f"\nğŸ’¾ Eksport wynikÃ³w...")

        # PeÅ‚ne dane
        csv_path = self.output_dir / "gtmo_full_analysis.csv"
        self.df.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"   âœ“ PeÅ‚na analiza: {csv_path.name}")

        # Podsumowanie JSON
        sa_nonzero = self.df[self.df['SA'] > 0]['SA']
        depth_nonzero = self.df[self.df['depth'] > 0]['depth']
        df_valid = self.df[self.df['SA'] > 0]

        critical_blocks = df_valid[df_valid['SA'] < CRITICAL_THRESHOLD]
        warning_blocks = df_valid[
            (df_valid['SA'] >= CRITICAL_THRESHOLD) &
            (df_valid['SA'] < WARNING_THRESHOLD)
        ]

        summary = {
            'document': self.json_path.parent.name,
            'total_blocks': len(self.df),
            'valid_blocks': len(df_valid),
            'critical_blocks': len(critical_blocks),
            'warning_blocks': len(warning_blocks),
            'statistics': {
                'mean_SA': float(sa_nonzero.mean()) if len(sa_nonzero) > 0 else 0,
                'min_SA': float(sa_nonzero.min()) if len(sa_nonzero) > 0 else 0,
                'max_SA': float(sa_nonzero.max()) if len(sa_nonzero) > 0 else 0,
                'std_SA': float(sa_nonzero.std()) if len(sa_nonzero) > 0 else 0,
                'mean_depth': float(depth_nonzero.mean()) if len(depth_nonzero) > 0 else 0,
                'max_depth': float(depth_nonzero.max()) if len(depth_nonzero) > 0 else 0
            }
        }

        json_path = self.output_dir / "gtmo_summary.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"   âœ“ Podsumowanie: {json_path.name}")


    def generate_all(self):
        """Generuje wszystkie analizy i wizualizacje"""
        self.print_statistics()
        self.visualize_ekg()
        self.identify_smoking_guns()
        self.visualize_chaos_heatmap()
        self.visualize_correlation()
        self.visualize_sa_distribution()
        self.export_results()

        print("\n" + "=" * 70)
        print("âœ… ANALIZA ZAKOÅƒCZONA")
        print("=" * 70)
        print(f"\nWszystkie wyniki zapisane w: {self.output_dir}")
        print(f"\nWygenerowane pliki:")
        for file in sorted(self.output_dir.iterdir()):
            print(f"   â€¢ {file.name}")


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='GTMÃ˜ Verdict Analyzer - Analizator WyrokÃ³w SÄ…dowych',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
PrzykÅ‚ady uÅ¼ycia:

  # PeÅ‚na analiza
  python gtmo_verdict_analyzer.py path/to/document.json --all

  # Tylko statystyki i smoking guns
  python gtmo_verdict_analyzer.py path/to/document.json --stats --smoking-guns

  # Wszystkie wizualizacje
  python gtmo_verdict_analyzer.py path/to/document.json --visualize
        """
    )

    parser.add_argument('json_file', help='ÅšcieÅ¼ka do pliku JSON z analizÄ… GTMÃ˜')

    # Opcje
    parser.add_argument('--all', action='store_true', help='Wszystkie analizy i wizualizacje')
    parser.add_argument('--stats', action='store_true', help='Statystyki ogÃ³lne')
    parser.add_argument('--ekg', action='store_true', help='Wykres "EKG" wyroku')
    parser.add_argument('--smoking-guns', action='store_true', help='Identyfikacja blokÃ³w krytycznych')
    parser.add_argument('--chaos', action='store_true', help='Mapa cieplna chaosu')
    parser.add_argument('--correlation', action='store_true', help='Macierz korelacji')
    parser.add_argument('--distribution', action='store_true', help='RozkÅ‚ad SA')
    parser.add_argument('--visualize', action='store_true', help='Wszystkie wizualizacje')
    parser.add_argument('--export', action='store_true', help='Eksport wynikÃ³w')
    parser.add_argument('--recommendations', action='store_true',
                       help='Generuj rekomendacje w jÄ™zyku naturalnym (z LLM)')
    parser.add_argument('--no-llm', action='store_true',
                       help='WyÅ‚Ä…cz LLM (szybsze, ale bez konkretnych przykÅ‚adÃ³w)')

    args = parser.parse_args()

    print("=" * 70)
    print("GTMÃ˜ VERDICT ANALYZER")
    print("=" * 70)

    analyzer = GTMOVerdictAnalyzer(args.json_file)

    if args.all:
        analyzer.generate_all()
    else:
        if args.stats:
            analyzer.print_statistics()
        if args.ekg or args.visualize:
            analyzer.visualize_ekg()
        if args.smoking_guns:
            analyzer.identify_smoking_guns()
        if args.chaos or args.visualize:
            analyzer.visualize_chaos_heatmap()
        if args.correlation or args.visualize:
            analyzer.visualize_correlation()
        if args.distribution or args.visualize:
            analyzer.visualize_sa_distribution()
        if args.export:
            analyzer.export_results()
        if args.recommendations:
            use_llm = not args.no_llm  # LLM domyÅ›lnie wÅ‚Ä…czony, chyba Å¼e --no-llm
            analyzer.generate_natural_language_recommendations(use_llm=use_llm)

        # JeÅ›li nic nie wybrano, pokaÅ¼ wszystko
        if not any([args.stats, args.ekg, args.smoking_guns, args.chaos,
                   args.correlation, args.distribution, args.visualize, args.export,
                   args.recommendations]):
            analyzer.generate_all()


if __name__ == "__main__":
    main()

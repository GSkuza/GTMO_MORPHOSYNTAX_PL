#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GTM√ò Comprehensive Test Runner
==============================
Unified test runner for all GTM√ò test suites including property-based tests.
"""

import sys
import os
import subprocess
import json
from pathlib import Path
import importlib.util

# Fix Windows console encoding (with error handling)
if sys.platform == 'win32':
    import io
    try:
        if not sys.stdout.closed:
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        if not sys.stderr.closed:
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except (ValueError, AttributeError):
        # Skip if streams are already wrapped or closed
        pass

class GTMOTestRunner:
    """Comprehensive test runner for GTM√ò."""
    
    def __init__(self):
        self.test_modules = [
            'test_basic.py',
            'test_constitutional_duality.py', 
            'test_diagnostyczny.py',
            'test_domain_dictionary.py',
            'test_property_based.py'
        ]
        self.results = {}
        
    def check_dependencies(self):
        """Check if all required dependencies are installed."""
        print("üîç Checking dependencies...")
        
        required_packages = [
            'hypothesis',
            'numpy', 
            'morfeusz2',
            'spacy'
        ]
        
        missing = []
        for package in required_packages:
            try:
                __import__(package)
                print(f"  ‚úì {package}")
            except ImportError:
                print(f"  ‚úó {package}")
                missing.append(package)
        
        if missing:
            print(f"\nüì¶ Installing missing packages: {', '.join(missing)}")
            for package in missing:
                if package == 'morfeusz2':
                    subprocess.run([sys.executable, '-m', 'pip', 'install', 'morfeusz2'])
                elif package == 'spacy':
                    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy'])
                    subprocess.run([sys.executable, '-m', 'spacy', 'download', 'pl_core_news_sm'])
                else:
                    subprocess.run([sys.executable, '-m', 'pip', 'install', package])
        
        return len(missing) == 0
    
    def run_module_tests(self, module_path):
        """Run tests from a specific module."""
        print(f"\nüìã Running tests from {module_path}")
        print("-" * 60)
        
        try:
            # Import and run the module
            spec = importlib.util.spec_from_file_location("test_module", module_path)
            test_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(test_module)
            
            # Look for main function or test functions
            if hasattr(test_module, 'main'):
                result = test_module.main()
                self.results[module_path] = {'status': 'passed' if result else 'failed', 'details': 'main() executed'}
            elif hasattr(test_module, 'run_all_tests'):
                result = test_module.run_all_tests()
                self.results[module_path] = {'status': 'passed' if result else 'failed', 'details': 'run_all_tests() executed'}
            else:
                # Look for functions starting with 'test_'
                test_functions = [name for name in dir(test_module) if name.startswith('test_')]
                passed = 0
                failed = 0
                
                for test_func_name in test_functions:
                    try:
                        test_func = getattr(test_module, test_func_name)
                        if callable(test_func):
                            test_func()
                            passed += 1
                            print(f"  ‚úì {test_func_name}")
                    except Exception as e:
                        failed += 1
                        print(f"  ‚úó {test_func_name}: {e}")
                
                total = passed + failed
                success_rate = (passed / total * 100) if total > 0 else 100
                self.results[module_path] = {
                    'status': 'passed' if success_rate >= 90 else 'failed',
                    'details': f'{passed}/{total} tests passed ({success_rate:.1f}%)'
                }
            
        except Exception as e:
            print(f"  ‚úó Failed to run module: {e}")
            self.results[module_path] = {'status': 'error', 'details': str(e)}
    
    def run_pytest_integration(self):
        """Run pytest with hypothesis integration if available."""
        print("\nüß™ Running pytest with Hypothesis integration...")
        
        try:
            # Create pytest configuration
            pytest_content = '''
# pytest configuration for GTM√ò
import pytest
from hypothesis import given, strategies as st, settings

# Import GTM√ò modules
from gtmo_morphosyntax import QuantumMorphosyntaxEngine

class TestGTMOIntegration:
    """Integration tests using pytest + hypothesis."""
    
    def setup_method(self):
        self.engine = QuantumMorphosyntaxEngine()
    
    @given(st.text(min_size=5, max_size=100))
    @settings(max_examples=20)
    def test_coordinates_bounded(self, text):
        """Test that coordinates are always bounded."""
        if not text.strip():
            return
            
        try:
            result = self.engine.gtmo_analyze_quantum(text)
            coords = result['coordinates']
            
            assert 0 <= coords['determination'] <= 1
            assert 0 <= coords['stability'] <= 1  
            assert 0 <= coords['entropy'] <= 1
        except:
            pass  # Skip invalid inputs
    
    @given(st.text(min_size=10, max_size=200))
    @settings(max_examples=15)
    def test_duality_preserved(self, text):
        """Test constitutional duality preservation."""
        if not text.strip() or len(text.split()) < 2:
            return
            
        try:
            result = self.engine.gtmo_analyze_quantum(text)
            duality = result['constitutional_metrics']['duality']
            error_percent = duality['error_percent']
            
            assert error_percent < 5.0, f"Duality error too high: {error_percent}%"
        except:
            pass
    
    def test_basic_functionality(self):
        """Test basic engine functionality."""
        test_text = "Rzeczpospolita Polska jest demokratycznym pa≈Ñstwem prawnym."
        result = self.engine.gtmo_analyze_quantum(test_text)
        
        assert 'coordinates' in result
        assert 'constitutional_metrics' in result
        assert 'quantum_metrics' in result
        
        # Check coordinate bounds
        coords = result['coordinates']
        assert 0 <= coords['determination'] <= 1
        assert 0 <= coords['stability'] <= 1
        assert 0 <= coords['entropy'] <= 1
        
        # Check duality
        duality = result['constitutional_metrics']['duality']
        assert duality['error_percent'] < 1.0
'''
            
            with open('test_integration_pytest.py', 'w', encoding='utf-8') as f:
                f.write(pytest_content)
            
            # Try to run pytest
            result = subprocess.run([sys.executable, '-m', 'pytest', 'test_integration_pytest.py', '-v'], 
                                  capture_output=True, text=True)
            
            if result.returncode == 0:
                print("  ‚úì pytest integration tests passed")
                self.results['pytest_integration'] = {'status': 'passed', 'details': 'All pytest tests passed'}
            else:
                print(f"  ‚úó pytest integration tests failed:\n{result.stdout}\n{result.stderr}")
                self.results['pytest_integration'] = {'status': 'failed', 'details': result.stderr}
                
        except FileNotFoundError:
            print("  ‚ö†Ô∏è  pytest not available, skipping integration tests")
            self.results['pytest_integration'] = {'status': 'skipped', 'details': 'pytest not installed'}
        except Exception as e:
            print(f"  ‚úó pytest integration error: {e}")
            self.results['pytest_integration'] = {'status': 'error', 'details': str(e)}
    
    def generate_coverage_report(self):
        """Generate test coverage report."""
        print("\nüìä Generating coverage report...")
        
        total_modules = len(self.test_modules)
        passed_modules = sum(1 for result in self.results.values() if result['status'] == 'passed')
        failed_modules = sum(1 for result in self.results.values() if result['status'] == 'failed')
        error_modules = sum(1 for result in self.results.values() if result['status'] == 'error')
        skipped_modules = sum(1 for result in self.results.values() if result['status'] == 'skipped')
        
        coverage_report = {
            'timestamp': '2025-11-05',
            'total_test_modules': total_modules + 1,  # +1 for pytest integration
            'passed': passed_modules,
            'failed': failed_modules,
            'errors': error_modules,
            'skipped': skipped_modules,
            'success_rate': (passed_modules / (total_modules + 1) * 100) if total_modules > 0 else 0,
            'detailed_results': self.results
        }
        
        # Save coverage report
        with open('test_coverage_report.json', 'w', encoding='utf-8') as f:
            json.dump(coverage_report, f, indent=2, ensure_ascii=False)
        
        print(f"üìà Coverage Summary:")
        print(f"  Total modules: {total_modules + 1}")
        print(f"  Passed: {passed_modules}")
        print(f"  Failed: {failed_modules}")
        print(f"  Errors: {error_modules}")
        print(f"  Skipped: {skipped_modules}")
        print(f"  Success rate: {coverage_report['success_rate']:.1f}%")
        
        return coverage_report
    
    def run_all_tests(self):
        """Run comprehensive test suite."""
        print("üöÄ GTM√ò COMPREHENSIVE TEST SUITE")
        print("=" * 80)
        
        # Check dependencies first
        if not self.check_dependencies():
            print("‚ùå Dependencies check failed")
            return False
        
        # Run module tests
        for module in self.test_modules:
            if Path(module).exists():
                self.run_module_tests(module)
            else:
                print(f"‚ö†Ô∏è  Module not found: {module}")
                self.results[module] = {'status': 'skipped', 'details': 'Module file not found'}
        
        # Run pytest integration
        self.run_pytest_integration()
        
        # Generate coverage report
        coverage = self.generate_coverage_report()
        
        # Final assessment
        success_rate = coverage['success_rate']
        overall_success = success_rate >= 90.0
        
        print("\n" + "=" * 80)
        print("üéØ FINAL ASSESSMENT")
        print("=" * 80)
        
        if overall_success:
            print("üéâ SUCCESS: GTM√ò achieves comprehensive test coverage!")
            print(f"   Test success rate: {success_rate:.1f}%")
            print("   ‚úì Property-based testing implemented")
            print("   ‚úì Golden dataset validation ready")
            print("   ‚úì Hypothesis integration working")
            print("   ‚úì Constitutional duality preserved")
            print("   ‚úì Quantum mechanics verified")
        else:
            print("‚ö†Ô∏è  NEEDS IMPROVEMENT")
            print(f"   Test success rate: {success_rate:.1f}% (target: 90%+)")
            print("   Review failed tests and improve coverage")
        
        return overall_success


def main():
    """Main entry point."""
    runner = GTMOTestRunner()
    return runner.run_all_tests()


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)